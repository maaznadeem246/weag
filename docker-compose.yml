# Auto-generated from scenario.toml
# Run: docker compose up --abort-on-container-exit

services:
  green-agent:
    image: weag-green-agent:latest
    platform: linux/amd64
    container_name: green-agent
    command: ["--host", "0.0.0.0", "--port", "9009", "--card-url", "http://green-agent:9009"]
    environment:
      - PYTHONUNBUFFERED=1
      - GREEN_LLM_PROVIDER=openai
      - GREEN_OPENAI_MODEL=gpt-4o
      - GREEN_OPENAI_API_KEY=${OPENAI_API_KEY}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9009/.well-known/agent-card.json"]
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 30s
    depends_on:
      purple_agent:
        condition: service_healthy
    networks:
      - agent-network

  purple_agent:
    image: weag-purple-agent:latest
    platform: linux/amd64
    container_name: purple_agent
    command: ["--host", "0.0.0.0", "--port", "9009", "--card-url", "http://purple_agent:9009"]
    environment:
      - PYTHONUNBUFFERED=1
      - PURPLE_LLM_PROVIDER=openai
      - PURPLE_OPENAI_MODEL=gpt-4o
      - PURPLE_OPENAI_API_KEY=${OPENAI_API_KEY}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9009/.well-known/agent-card.json"]
      interval: 5s
      timeout: 3s
      retries: 10
      start_period: 30s
    networks:
      - agent-network

  agentbeats-client:
    image: ghcr.io/agentbeats/agentbeats-client:v1.0.0
    platform: linux/amd64
    container_name: agentbeats-client
    volumes:
      - ./a2a-scenario.toml:/app/scenario.toml
      - ./output:/app/output
    command: ["scenario.toml", "output/results.json"]
    depends_on:
      green-agent:
        condition: service_healthy
      purple_agent:
        condition: service_healthy
    networks:
      - agent-network

networks:
  agent-network:
    driver: bridge
