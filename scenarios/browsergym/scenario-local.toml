# Local Development Scenario for kickstart_assessment.py
# This is the default config for local (non-Docker) testing.
#
# ⚠️  CONFIGURATION POLICY:
#     - API keys and secrets: .env file ONLY (never commit to git)
#     - All other settings: THIS TOML file
#     - Runtime overrides: CLI flags (--task, --visible, --log-level, etc.)
#
# USAGE:
#   python kickstart_assessment.py                      # Uses this config
#   python kickstart_assessment.py --task miniwob.click-test  # Single task override
#   python kickstart_assessment.py --visible            # Override headless mode
#   python kickstart_assessment.py --log-level DEBUG    # Override logging
#
# CLI flags override these defaults.

# =============================================================================
# CONFIGURATION - All assessment settings centralized here
# =============================================================================
[config]
# Domain-specific settings
domain = "browsergym"  # benchmark domain: browsergym, airline, etc.

# Assessment execution settings
timeout_seconds = 120  # Default timeout for the whole assessment run (seconds)
max_steps = 10  # Default max steps per task (passed to Green Agent)
max_tasks_per_benchmark = 2  # Default max tasks per benchmark
headless = false  # Browser visibility (false = visible, true = headless)

# Optional: Override max tasks for all benchmarks
# num_tasks = 3

# =============================================================================
# GREEN AGENT (Evaluator) - Configure endpoint and model here
# =============================================================================
[green_agent]
# AgentBeats platform registration ID (for GitHub Actions deployment)
agentbeats_id = ""  # Your green agent ID from AgentBeats

# Local endpoint (host:port where Green Agent runs)
endpoint = "http://127.0.0.1:9009"
# Command to start Green Agent subprocess (auto-spawned by kickstart script)
cmd = "python -m src.green_agent.main --host 127.0.0.1 --port 9009"

# Environment variables - API keys injected from .env or GitHub Actions secrets
# Use ${VAR_NAME} syntax for dynamic injection (GitHub Actions, CI/CD)
env = {
    GREEN_LLM_PROVIDER = "openai",
    GREEN_OPENAI_MODEL = "gpt-4o",
    GREEN_OPENAI_API_KEY = "${OPENAI_API_KEY}",
    # GREEN_GEMINI_MODEL = "gemini-2.5-flash",
    # GREEN_GEMINI_API_KEY = "${GEMINI_API_KEY}",
    # GREEN_LITELLM_MODEL = "google/gemini-2.0-flash-exp:free",
    # GREEN_OPENROUTER_API_KEY = "${OPENROUTER_API_KEY}",
    # LANGFUSE_PUBLIC_KEY = "${LANGFUSE_PUBLIC_KEY}",
    # LANGFUSE_SECRET_KEY = "${LANGFUSE_SECRET_KEY}",
}

# =============================================================================
# PURPLE AGENT (Participant) - Configure endpoint and model here
# =============================================================================
[[participants]]
# AgentBeats platform registration ID (for GitHub Actions deployment)
agentbeats_id = ""  # Your purple agent ID from AgentBeats
name = "purple_agent"
role = "purple_agent"

# Local endpoint (where Purple Agent runs)
endpoint = "http://127.0.0.1:9010/"
# Command to start Purple Agent subprocess (auto-spawned by kickstart script)
cmd = "python -m src.purple_agent.main --port 9010"

# Environment variables - API keys injected from .env or GitHub Actions secrets
# Use ${VAR_NAME} syntax for dynamic injection (GitHub Actions, CI/CD)
env = {
    PURPLE_LLM_PROVIDER = "openai",
    PURPLE_OPENAI_MODEL = "gpt-4o",
    PURPLE_OPENAI_API_KEY = "${OPENAI_API_KEY}",
    # PURPLE_GEMINI_MODEL = "gemini-2.5-flash",
    # PURPLE_GEMINI_API_KEY = "${GEMINI_API_KEY}",
    # PURPLE_LITELLM_MODEL = "openai/gpt-oss-120b",
    # PURPLE_OPENROUTER_API_KEY = "${OPENROUTER_API_KEY}",
    # LANGFUSE_PUBLIC_KEY = "${LANGFUSE_PUBLIC_KEY}",
    # LANGFUSE_SECRET_KEY = "${LANGFUSE_SECRET_KEY}",
}

# =============================================================================
# BENCHMARKS CONFIGURATION
# =============================================================================
# Each [[benchmarks]] section defines a benchmark to evaluate.
# 
# Options:
#   id        - Benchmark identifier (miniwob, assistantbench, webarena, etc.)
#   max_tasks - Max tasks to run for THIS benchmark (overrides global max_tasks_per_benchmark)
#   tasks     - OPTIONAL explicit task list. If OMITTED, auto-discovers available tasks!
#
# AUTO-DISCOVERY MODE (RECOMMENDED):
#   Simply omit the `tasks` line - system will find all available tasks and limit by max_tasks
#
# SUPPORTED BENCHMARKS:
#   - miniwob: Requires local dataset at benchmarks/miniwob/html/miniwob/
#   - assistantbench: Auto-downloads from HuggingFace (no setup needed)
#   - webarena: Requires Docker containers (WA_* env vars)
#   - visualwebarena: Requires Docker containers (VWA_* env vars)
#   - workarena: Requires ServiceNow dev instance (WORKARENA_* env vars)
#   - weblinx: Requires dataset download (WEBLINX_DATA_PATH env var)

# MiniWoB - Local HTML tasks (ready to use)
# [[benchmarks]]
# id = "miniwob"
# Uses global max_tasks_per_benchmark

# AssistantBench - HuggingFace tasks (ready to use)
# [[benchmarks]]
# id = "assistantbench"

# WebArena - Requires Docker setup (comment out if not available)
# [[benchmarks]]
# id = "webarena"
# max_tasks = 3

# VisualWebArena - Requires Docker setup (comment out if not available)
# [[benchmarks]]
# id = "visualwebarena"
# max_tasks = 3

# WorkArena - Requires ServiceNow instance (comment out if not available)
# [[benchmarks]]
# id = "workarena"
# max_tasks = 3

# WebLINX - Requires dataset download (comment out if not available)
# [[benchmarks]]
# id = "weblinx"
# max_tasks = 3

# EXPLICIT MODE EXAMPLE (uncomment to use specific tasks):
# [[benchmarks]]
# id = "miniwob"
# tasks = ["click-test", "ascending-numbers", "book-flight"]
