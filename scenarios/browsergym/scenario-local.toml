# Local Development Scenario for kickstart_assessment.py
# This is the default config for local (non-Docker) testing.
#
# ⚠️  CONFIGURATION POLICY:
#     - API keys and secrets: .env file ONLY (never commit to git)
#     - All other settings: THIS TOML file
#     - Runtime overrides: CLI flags (--task, --visible, --log-level, etc.)
#
# USAGE:
#   python kickstart_assessment.py                      # Uses this config
#   python kickstart_assessment.py --task miniwob.click-test  # Single task override
#   python kickstart_assessment.py --visible            # Override headless mode
#   python kickstart_assessment.py --log-level DEBUG    # Override logging
#
# CLI flags override these defaults.

# =============================================================================
# CONFIGURATION - All assessment settings centralized here
# =============================================================================
[config]
# Domain-specific settings
domain = "browsergym"  # benchmark domain: browsergym, airline, etc.

# Assessment execution settings
timeout_seconds = 120  # Default timeout for the whole assessment run (seconds)
max_steps = 10  # Default max steps per task (passed to Green Agent)
max_tasks_per_benchmark = 2  # Default max tasks per benchmark
headless = false  # Browser visibility (false = visible, true = headless)

# Optional: Override max tasks for all benchmarks
# num_tasks = 3

# =============================================================================
# GREEN AGENT (Evaluator) - Configure endpoint and model here
# =============================================================================
[green_agent]
# AgentBeats platform registration ID (for GitHub Actions deployment)
agentbeats_id = ""  # Your green agent ID from AgentBeats

# Local endpoint (host:port where Green Agent runs)
endpoint = "http://127.0.0.1:9009"
# Command to start Green Agent subprocess (auto-spawned by kickstart script)
cmd = "python -m src.green_agent.main --host 127.0.0.1 --port 9009"

# Environment variables - API keys injected from .env or GitHub Actions secrets
# Use ${VAR_NAME} syntax for dynamic injection (GitHub Actions, CI/CD)
# IMPORTANT: Inline tables must be on ONE line with NO comments inside
env = { GREEN_LLM_PROVIDER = "litellm", GREEN_LITELLM_MODEL = "google/gemini-2.5-pro", GREEN_OPENROUTER_API_KEY = "${OPENROUTER_API_KEY}" }

# =============================================================================
# PURPLE AGENT (Participant) - Configure endpoint and model here
# =============================================================================
[[participants]]
# AgentBeats platform registration ID (for GitHub Actions deployment)
agentbeats_id = ""  # Your purple agent ID from AgentBeats
name = "purple_agent"
role = "purple_agent"

# Local endpoint (where Purple Agent runs)
endpoint = "http://127.0.0.1:9010/"
# Command to start Purple Agent subprocess (auto-spawned by kickstart script)
cmd = "python -m src.purple_agent.main --port 9010"

# Environment variables - API keys injected from .env or GitHub Actions secrets
# Use ${VAR_NAME} syntax for dynamic injection (GitHub Actions, CI/CD)
# Note: Inline tables must be on ONE line with no comments
env = { PURPLE_LLM_PROVIDER = "litellm", PURPLE_LITELLM_MODEL = "google/gemini-2.5-pro", PURPLE_OPENROUTER_API_KEY = "${OPENROUTER_API_KEY}" }

# =============================================================================
# BENCHMARKS CONFIGURATION
# =============================================================================
# Each [[benchmarks]] section defines a benchmark to evaluate.


# MiniWoB - Local HTML tasks (ready to use)
# [[benchmarks]]
# id = "miniwob"
# Uses global max_tasks_per_benchmark

# AssistantBench - HuggingFace tasks (ready to use)
# [[benchmarks]]
# id = "assistantbench"
# Uses global max_tasks_per_benchmark

# WebArena - Requires Docker setup (comment out if not available)
# [[benchmarks]]
# id = "webarena"
# max_tasks = 3

# VisualWebArena - Requires Docker setup (comment out if not available)
# [[benchmarks]]
# id = "visualwebarena"
# max_tasks = 3

# WorkArena - Requires ServiceNow instance (comment out if not available)
# [[benchmarks]]
# id = "workarena"
# max_tasks = 3

# WebLINX - Requires dataset download (comment out if not available)
# [[benchmarks]]
# id = "weblinx"
# max_tasks = 3

# EXPLICIT MODE EXAMPLE (uncomment to use specific tasks):
# [[benchmarks]]
# id = "miniwob"
# tasks = ["click-test", "ascending-numbers", "book-flight"]
